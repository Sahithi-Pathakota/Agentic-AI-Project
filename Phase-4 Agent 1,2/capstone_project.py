# -*- coding: utf-8 -*-
"""Capstone Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/102BTae1SLDfL3ZjbANN9RPvPPr0voSzn
"""

pip install pandas numpy matplotlib

!pip install pandas numpy matplotlib

pip install tabulate

!pip install redis --quiet

!pip install streamlit pyngrok nest_asyncio redis pandas numpy matplotlib tabulate -q

from pyngrok import ngrok
ngrok.kill()
!pkill -f streamlit || echo "no old streamlit"

!nohup streamlit run /content/streamlit_sentinel.py --server.port 8501 --server.headless true > /content/streamlit.log 2>&1 &

"""
Agent1: Network Monitoring Agent (Generator + Baseline Profiler)
Notebook-friendly version without Redis. Publishes events to an async queue.
"""

import asyncio
import json
import time
import random
from statistics import mean, pstdev

# ---------------- CONFIG ----------------
NUM_NODES = 8
BASELINE_SECONDS = 5        # shortened for demo
POST_BASELINE_SECONDS = 8
EVENT_INTERVAL = 0.05       # seconds between publishing a batch
ATTACK_NODE_INDEX = 3
ATTACK_FACTOR = 10

METRICS = ["outbound_conn", "failed_login", "data_transfer_kb"]
random.seed(42)

# ---------------- Event Bus ----------------
class EventBus:
    """Simple async in-memory pub/sub queue."""
    def __init__(self):
        self.queue = asyncio.Queue()

    async def publish(self, event):
        await self.queue.put(event)

    async def subscribe(self):
        while True:
            ev = await self.queue.get()
            yield ev

# ---------------- Generator ----------------
class Generator:
    def __init__(self, bus: EventBus):
        self.node_ids = [f"node-{i:02d}" for i in range(NUM_NODES)]
        self.truth = {
            n: {
                "outbound_conn": (random.randint(1,5), 1.0),
                "failed_login": (random.choice([0,0,1]), 0.5),
                "data_transfer_kb": (random.uniform(50,400), 30.0)
            } for n in self.node_ids
        }
        self.attack_start_ts = {}
        self.bus = bus

    async def publish(self, ev):
        await self.bus.publish(ev)

    async def run_baseline(self):
        print("[Agent1] Running baseline...")
        collected = []
        t0 = time.time()
        while time.time() - t0 < BASELINE_SECONDS:
            ts = time.time()
            for n in self.node_ids:
                for metric in METRICS:
                    mu, sd = self.truth[n][metric]
                    val = max(0.0, random.gauss(mu, sd))
                    ev = {"ts": ts, "node": n, "metric": metric, "value": val, "meta": {}}
                    await self.publish(ev)
                    collected.append(ev)
            await asyncio.sleep(EVENT_INTERVAL)
        profile = self.compute_profile(collected)
        with open("baseline_profile.json", "w") as f:
            json.dump(profile, f, indent=2)
        print("[Agent1] Baseline computed & saved locally.")
        return profile

    def compute_profile(self, events):
        agg = {}
        for ev in events:
            n = ev["node"]; m = ev["metric"]; v = ev["value"]
            agg.setdefault(n, {}).setdefault(m, []).append(v)
        profile = {}
        for n, metrics in agg.items():
            profile[n] = {}
            for m, vals in metrics.items():
                mu = mean(vals) if vals else 0.0
                sd = pstdev(vals) if len(vals)>1 else 0.0
                sd = max(sd, 1e-3)
                profile[n][m] = {"mean": mu, "std": sd}
        return profile

    async def run_post_baseline(self):
        print("[Agent1] Running post-baseline (attack insertion)...")
        attack_inserted = False
        t0 = time.time()
        while time.time() - t0 < POST_BASELINE_SECONDS:
            ts = time.time()
            for idx, n in enumerate(self.node_ids):
                for metric in METRICS:
                    mu, sd = self.truth[n][metric]
                    val = random.gauss(mu, sd)
                    meta = {}
                    # insert attack at roughly 1/4 of post-baseline
                    if (not attack_inserted) and ((time.time()-t0) > POST_BASELINE_SECONDS/4) and idx==ATTACK_NODE_INDEX:
                        attack_inserted = True
                        attack_ts = time.time()
                        self.attack_start_ts[n] = attack_ts
                        meta["attack_start"] = True
                        # simulate strong anomalous values
                        if metric == "outbound_conn":
                            val = max(0.0, mu * ATTACK_FACTOR + abs(random.gauss(0, sd*2)))
                        elif metric == "failed_login":
                            val = max(0.0, mu + 20 + abs(random.gauss(0,3)))
                        elif metric == "data_transfer_kb":
                            val = max(0.0, mu * (ATTACK_FACTOR/2) + abs(random.gauss(0, sd*3)))
                        print(f"[Agent1] Attack inserted on {n} at {attack_ts:.3f}")
                    ev = {"ts": ts, "node": n, "metric": metric, "value": max(0.0, val), "meta": meta}
                    await self.publish(ev)
            await asyncio.sleep(EVENT_INTERVAL)
        print("[Agent1] Post-baseline finished.")
        return self.attack_start_ts

# ---------------- Main ----------------
async def main():
    bus = EventBus()
    gen = Generator(bus)
    await gen.run_baseline()
    attack_times = await gen.run_post_baseline()
    await gen.publish({"type": "GEN_DONE"})
    print("[Agent1] Done. Attack times:", attack_times)

# ---------------- Run in Notebook ----------------
# Just run this cell in a notebook:
await main()

#!/usr/bin/env python3
"""
Agent2: Anomaly Detector (Notebook-friendly)
Subscribes to "sentinel:raw_events", loads baseline from Redis,
computes z-score → severity, prints and optionally publishes alerts.
"""

import asyncio, json, time, os
from statistics import mean
import redis.asyncio as redis
from collections import defaultdict, deque

# ---------------- CONFIG ----------------
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
RAW_CHANNEL = "sentinel:raw_events"
ALERT_CHANNEL = "sentinel:alerts"
BASELINE_KEY = "sentinel:baseline"
SEVERITY_CRITICAL = 85

def severity_from_z(z, metric):
    weight = 1.5 if metric=="outbound_conn" else 1.0
    raw = min(100.0, max(0.0, abs(z)*15*weight))
    return raw

# ---------------- DETECTOR CLASS ----------------
class Detector:
    def __init__(self, r):
        self.r = r
        self.window = defaultdict(lambda: defaultdict(lambda: deque(maxlen=6)))
        self.baseline = {}
        self.last_alert = {}

    async def load_baseline(self):
        b = await self.r.get(BASELINE_KEY)
        while not b:
            print("[Agent2] Waiting for baseline in Redis...")
            await asyncio.sleep(0.5)
            b = await self.r.get(BASELINE_KEY)
        self.baseline = json.loads(b)
        print("[Agent2] Baseline loaded from Redis.")

    async def handle_event(self, ev):
        if ev.get("type") == "GEN_DONE":
            print("[Agent2] Generator finished signal received.")
            return
        node = ev["node"]
        metric = ev["metric"]
        val = ev["value"]
        dq = self.window[node][metric]
        dq.append(val)
        cur = mean(dq)
        if node in self.baseline and metric in self.baseline[node]:
            mu = self.baseline[node][metric]["mean"]
            sd = self.baseline[node][metric]["std"]
            z = (cur - mu) / sd
            severity = severity_from_z(z, metric)
            if severity >= SEVERITY_CRITICAL:
                now = time.time()
                last = self.last_alert.get(node, 0)
                if now - last > 1.5:
                    self.last_alert[node] = now
                    alert = {
                        "node": node,
                        "anomaly_type": metric,
                        "severity": severity,
                        "detection_ts": now,
                        "reason": f"{metric} z={z:.2f} (cur={cur:.2f}, μ={mu:.2f}, σ={sd:.2f})"
                    }
                    # print alert to notebook console
                    print(f"[Agent2] ALERT: {node} ({metric}) sev={severity:.1f}")
                    # optionally publish to Redis alerts channel
                    await self.r.publish(ALERT_CHANNEL, json.dumps(alert))

    async def run(self):
        await self.load_baseline()
        sub = self.r.pubsub()
        await sub.subscribe(RAW_CHANNEL)
        print("[Agent2] Subscribed to raw events.")
        async for message in sub.listen():
            if message is None:
                await asyncio.sleep(0.1)
                continue
            if message.get("type") != "message":
                continue
            data = message.get("data")
            try:
                ev = json.loads(data)
            except Exception:
                continue
            await self.handle_event(ev)

# ---------------- MAIN FOR NOTEBOOK ----------------
# In a notebook, do NOT use asyncio.run()
# Use: await main() in a cell

async def main():
    r = redis.from_url(REDIS_URL)
    det = Detector(r)
    try:
        await det.run()
    finally:
        await r.close()

#!/usr/bin/env python3
"""
Agent3: Response & Quarantine
Subscribes to "sentinel:alerts", updates node status in Redis hash "sentinel:node_status",
records containment times in Redis key "sentinel:containment" and prints dashboard.
"""

import asyncio, json, time, os
import redis.asyncio as redis
import nest_asyncio  # ✅ For Jupyter/Colab compatibility

# Allow nested event loops (Colab/Jupyter fix)
nest_asyncio.apply()

# Redis setup
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
ALERT_CHANNEL = "sentinel:alerts"
NODE_STATUS_HASH = "sentinel:node_status"
CONTAINMENT_HASH = "sentinel:containment"
ATTACK_START_KEY = "sentinel:attack_start"  # optional; Agent1 may set this


# ---------------- Quarantine Logic ----------------
async def quarantine(r, node, alert):
    """Simulate node isolation and record containment time."""
    await asyncio.sleep(0.08)
    now = time.time()

    # mark node as isolated
    await r.hset(NODE_STATUS_HASH, node, "ISOLATED")

    # compute containment time
    attack_start = await r.hget(ATTACK_START_KEY, node)
    if attack_start:
        try:
            atk_ts = float(attack_start)
            containment = now - atk_ts
        except Exception:
            containment = now - alert.get("detection_ts", now)
    else:
        containment = now - alert.get("detection_ts", now)

    await r.hset(CONTAINMENT_HASH, node, str(containment))
    print(f"[Agent3] ✅ Quarantined {node}; containment_time={containment:.3f}s")


# ---------------- Dashboard Logic ----------------
async def dashboard_loop(r):
    """Displays node status dashboard."""
    while True:
        all_status = await r.hgetall(NODE_STATUS_HASH)
        containment = await r.hgetall(CONTAINMENT_HASH)
        now = time.time()
        print("=" * 60)
        print(f"🛡 Sentinel Dashboard (t={now:.1f})")

        if not all_status:
            print("No node status yet. (Waiting for alerts...)")
        else:
            for node_b, st_b in all_status.items():
                node = node_b.decode() if isinstance(node_b, bytes) else node_b
                st = st_b.decode() if isinstance(st_b, bytes) else st_b
                note = ""
                ct_b = containment.get(node_b)
                if ct_b:
                    note = f"(contained in {float(ct_b):.3f}s)"
                print(f"  {node:10} | {st:10} {note}")
        print("=" * 60)
        await asyncio.sleep(2.0)


# ---------------- Main Event Loop ----------------
async def main():
    r = redis.from_url(REDIS_URL)
    sub = r.pubsub()
    await sub.subscribe(ALERT_CHANNEL)
    print("[Agent3] 🔔 Subscribed to alert channel.")

    # Run dashboard concurrently
    asyncio.create_task(dashboard_loop(r))

    async for msg in sub.listen():
        if msg is None:
            await asyncio.sleep(0.1)
            continue
        if msg.get("type") != "message":
            continue

        try:
            alert = json.loads(msg.get("data"))
        except Exception as e:
            print(f"[Agent3] ⚠️ Error parsing alert: {e}")
            continue

        node = alert.get("node")
        cur_status = await r.hget(NODE_STATUS_HASH, node)
        if cur_status and (cur_status.decode() if isinstance(cur_status, bytes) else cur_status) == "ISOLATED":
            print(f"[Agent3] 🚫 Alert for {node} ignored; already isolated.")
            continue

        await quarantine(r, node, alert)


# ---------------- Run Section ----------------
try:
    loop = asyncio.get_event_loop()
    if loop.is_running():
        print("[Agent3] ⚙️ Running inside an interactive environment...")
        task = loop.create_task(main())
    else:
        asyncio.run(main())
except KeyboardInterrupt:
    print("Agent3 stopped.")

!pip install streamlit pyngrok nest_asyncio -q

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/streamlit_sentinel.py
# import streamlit as st
# import subprocess
# import os
# 
# st.set_page_config(page_title="🛡 Sentinel Network Dashboard", layout="wide")
# st.title("🛡 Sentinel AI Agent System (Agent1, Agent2, Agent3)")
# 
# st.markdown("""
# ### Overview
# This Streamlit dashboard runs and monitors:
# - *Agent1* → Network Event Generator & Baseline Profiler
# - *Agent2* → Anomaly Detector (Z-score based)
# - *Agent3* → Response & Quarantine Manager
# """)
# 
# col1, col2 = st.columns(2)
# 
# if col1.button("▶ Start Agent1 (Generator)"):
#     subprocess.Popen(["python3", "/content/agent1_generator.py"])
#     st.success("Agent1 started successfully!")
# 
# if col1.button("▶ Start Agent2 (Detector)"):
#     subprocess.Popen(["python3", "/content/agent2_detector.py"])
#     st.success("Agent2 started successfully!")
# 
# if col1.button("▶ Start Agent3 (Responder)"):
#     subprocess.Popen(["python3", "/content/agent3_responder.py"])
#     st.success("Agent3 started successfully!")
# 
# st.divider()
# st.markdown("### 🧠 Live Status Logs")
# log_file = "/content/streamlit.log"
# if os.path.exists(log_file):
#     with open(log_file, "r") as f:
#         logs = f.read()[-1500:]
#         st.text_area("Log Output", logs, height=300)
# else:
#     st.info("No logs yet. Start an agent first.")

import subprocess

# Run Agent1
subprocess.Popen(["python3", "/content/agent1_generator.py"])

# Run Agent2
subprocess.Popen(["python3", "/content/agent2_detector.py"])

# Run Agent3
subprocess.Popen(["python3", "/content/agent3_responder.py"])

from pyngrok import ngrok
ngrok.kill()

!pkill -f streamlit || echo "no old streamlit process"

!apt-get install redis-server -y
!redis-server --daemonize yes

!redis-cli ping

import redis
r = redis.Redis(host='localhost', port=6379, db=0)
print("✅ Redis connected:", r.ping())

import nest_asyncio
nest_asyncio.apply()

from pyngrok import ngrok

NGROK_AUTH_TOKEN = "32uRG6HX54VoU0hLdgmV5keYvYJ_4Q55SPHcDg7UjBr8MQkyD"  # replace with your token
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Start Streamlit (in background) if not already started
!nohup streamlit run /content/streamlit_sentinel.py --server.port 8501 --server.headless true > /content/streamlit.log 2>&1 &

# Start ngrok tunnel
tunnel = ngrok.connect(8501)
print("🌍 Public Streamlit URL:", tunnel.public_url)

from pyngrok import ngrok
ngrok.set_auth_token("32uRG6HX54VoU0hLdgmV5keYvYJ_4Q55SPHcDg7UjBr8MQkyD")
tunnel = ngrok.connect(8501)
print("Public URL:", tunnel.public_url)

!tail -n 20 /content/streamlit.log